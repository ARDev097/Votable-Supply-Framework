{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "    os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"../../Dataset/Historical_Data/Liquidity_Activity_Ratio/LAR_Historical_Data.csv\")\n",
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Extract and scale data\n",
    "timeseries_data = data['LAR'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "timeseries_data_scaled = scaler.fit_transform(timeseries_data)\n",
    "\n",
    "def prepare_data(series, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - n_steps):\n",
    "        X.append(series[i:i + n_steps])\n",
    "        y.append(series[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "def generate_future_predictions(model, last_sequence, n_steps, n_features, scaler, \n",
    "                              batch_size=10, end_date=datetime.datetime(2026, 12, 31), \n",
    "                              retrain=True):\n",
    "    predictions_dict = {}  \n",
    "    current_date = data.index[-1]\n",
    "    \n",
    "    current_sequence = last_sequence.reshape(1, n_steps, n_features)\n",
    "    all_data_scaled = timeseries_data_scaled.copy()\n",
    "    \n",
    "    while current_date < end_date:\n",
    "        batch_predictions = []\n",
    "        batch_dates = []\n",
    "        \n",
    "        # Generate predictions for the batch\n",
    "        for _ in range(batch_size):\n",
    "            current_date += pd.DateOffset(days=1)\n",
    "            if current_date > end_date:\n",
    "                break\n",
    "                \n",
    "            # Predict next value\n",
    "            next_pred = model.predict(current_sequence, verbose=0)[0, 0]\n",
    "            \n",
    "            # Store prediction and date\n",
    "            batch_predictions.append(next_pred)\n",
    "            batch_dates.append(current_date)\n",
    "            \n",
    "            # Update sequence for next prediction\n",
    "            current_sequence = np.roll(current_sequence, -1, axis=1)\n",
    "            current_sequence[0, -1, 0] = next_pred\n",
    "        \n",
    "        if not batch_predictions:  # If no predictions were made, break\n",
    "            break\n",
    "            \n",
    "        # Convert predictions back to original scale\n",
    "        batch_predictions = np.array(batch_predictions).reshape(-1, 1)\n",
    "        batch_predictions_inv = scaler.inverse_transform(batch_predictions)\n",
    "        \n",
    "        # Store predictions in dictionary\n",
    "        for date, pred in zip(batch_dates, batch_predictions_inv.flatten()):\n",
    "            predictions_dict[date] = pred\n",
    "        \n",
    "        if retrain:\n",
    "            # Add new predictions to training data\n",
    "            all_data_scaled = np.vstack([all_data_scaled, batch_predictions])\n",
    "            \n",
    "            # Prepare new training data\n",
    "            X_new, y_new = prepare_data(all_data_scaled, n_steps)\n",
    "            X_new = X_new.reshape((X_new.shape[0], X_new.shape[1], n_features))\n",
    "            \n",
    "            # Retrain model\n",
    "            model.fit(X_new, y_new, epochs=5, batch_size=16, verbose=0)\n",
    "            print(f\"Model retrained - Current date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Convert dictionary to series\n",
    "    predictions_series = pd.Series(predictions_dict, name='Predicted_LAR')\n",
    "    predictions_series.index.name = 'Date'\n",
    "    return predictions_series\n",
    "\n",
    "# Prepare data for full training\n",
    "n_steps = 16  # 60-day window\n",
    "n_features = 1\n",
    "X, y = prepare_data(timeseries_data_scaled, n_steps)\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "# Train model on full dataset\n",
    "print(\"Training model on complete dataset...\")\n",
    "model = build_lstm_model((n_steps, n_features))\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X, y, epochs=150, batch_size=16, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Generate predictions until December 31st, 2026\n",
    "print(\"\\nGenerating predictions until December 31st, 2026...\")\n",
    "last_sequence = timeseries_data_scaled[-n_steps:]\n",
    "future_predictions = generate_future_predictions(\n",
    "    model=model,\n",
    "    last_sequence=last_sequence,\n",
    "    n_steps=n_steps,\n",
    "    n_features=n_features,\n",
    "    scaler=scaler,\n",
    "    batch_size=10,  # 30-day batches\n",
    "    end_date=datetime.datetime(2026, 12, 31),\n",
    "    retrain=True\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(data.index, data['LAR'], label='Historical LAR', color='blue')\n",
    "plt.plot(future_predictions.index, future_predictions.values, \n",
    "         label='Predicted LAR', color='red', linestyle='--')\n",
    "plt.axvline(x=data.index[-1], color='green', linestyle=':', label='Prediction Start')\n",
    "plt.title('LAR: Historical and Predicted Values until 2026')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('LAR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "output_df = pd.DataFrame(future_predictions)\n",
    "output_df.reset_index(inplace=True)\n",
    "output_df.to_csv(\"../../Dataset/Prediction_Data/Liquidity_Activity_Ratio/LAR_Future_Data.csv\", index=False)\n",
    "print(\"\\nPredictions saved to '../../Dataset/Prediction_Data/Liquidity_Activity_Ratio/LAR_Future_Data.csv'\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nPrediction Summary:\")\n",
    "print(f\"Prediction Start Date: {data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Prediction End Date: {future_predictions.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Number of days predicted: {len(future_predictions)}\")\n",
    "print(f\"Initial LAR: {data['LAR'].iloc[-1]:.2f}\")\n",
    "print(f\"Final Predicted LAR: {future_predictions.iloc[-1]:.2f}\")\n",
    "print(f\"Minimum Predicted LAR: {future_predictions.min():.2f}\")\n",
    "print(f\"Maximum Predicted LAR: {future_predictions.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
